{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134898f6-d2fe-4010-855b-ef27c6d42019",
   "metadata": {},
   "source": [
    "# NCAR BCSD AK indicators processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef48f1d0-686d-4a1f-af55-0d8511be963c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indicators.py\n",
    "# TO-DO: put this in a separate script\n",
    "\"\"\"This script includes functions that define the various extreme variables we will by deriving\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import xclim.indices as xci\n",
    "from xclim.core.calendar import percentile_doy\n",
    "from xclim.core.units import convert_units_to, to_agg_units\n",
    "from xclim.indices.generic import threshold_count\n",
    "\n",
    "\n",
    "def take_sorted(arr, axis, idx):\n",
    "    \"\"\"Helper function for the 'hot day' and 'cold day' indices to slice a numpy array after sorting it. Done in favor of fixed, []-based indexing.\n",
    "    \n",
    "    Args:\n",
    "        arr (numpy.ndarray): array\n",
    "        axis (int): axis to sort and slice according to\n",
    "        idx (int): index value to slice arr at across all other axes\n",
    "        \n",
    "    Returns:\n",
    "        array of values at position idx of arr sorted along axis\n",
    "    \"\"\"\n",
    "    return np.take(np.sort(arr, axis), idx, axis)\n",
    "\n",
    "\n",
    "def hd(tasmax):\n",
    "    \"\"\"'Hot Day' - the 6th hottest day of the year\n",
    "    \n",
    "    Args:\n",
    "        tasmax (xarray.DataArray): daily maximum temperature values for a year\n",
    "        \n",
    "    Returns:\n",
    "        Hot Day values for each year\n",
    "    \"\"\"\n",
    "    def func(tasmax):\n",
    "        # np.sort defaults to ascending.. \n",
    "        #   hd is simply \"6th hottest\" day\n",
    "        return tasmax.reduce(take_sorted, dim=\"time\", idx=-6)\n",
    "    \n",
    "    # hardcoded unit conversion\n",
    "    out = tasmax.resample(time=\"1Y\").map(func) - 273.15\n",
    "    out.attrs[\"units\"] = \"C\"\n",
    "    out.attrs[\"comment\"] = \"'hot day': 6th hottest day of the year\"\n",
    "    \n",
    "    return out\n",
    "    \n",
    "\n",
    "def cd(tasmin):\n",
    "    \"\"\"'Cold Day' - the 6th coldest day of the year\n",
    "    \n",
    "    Args:\n",
    "        tasmin (xarray.DataArray): daily minimum temperature values\n",
    "        \n",
    "    Returns:\n",
    "        Cold Day values for each year\n",
    "    \"\"\"\n",
    "    def func(tasmin):\n",
    "        # time_ax = np.where(np.array(tasmin.dims) == \"time\")[0][0]\n",
    "        # np.sort defaults to ascending.. \n",
    "        #   cd is simply \"6th coldest\" day\n",
    "        return tasmin.reduce(take_sorted, dim=\"time\", idx=5)\n",
    "    \n",
    "    # hardcoded unit conversion\n",
    "    out = tasmin.resample(time=\"1Y\").map(func) - 273.15\n",
    "    out.attrs[\"units\"] = \"C\"\n",
    "    out.attrs[\"comment\"] = \"'cold day': 6th coldest day of the year\"\n",
    "    \n",
    "    return out\n",
    "    \n",
    "\n",
    "def rx1day(pr):\n",
    "    \"\"\"'Max 1-day precip' - the max daily precip value recorded for a year.\n",
    "    \n",
    "    Args:\n",
    "        pr (xarray.DataArray): daily total precip values\n",
    "        \n",
    "    Returns:\n",
    "        Max 1-day precip for each year\n",
    "    \"\"\"\n",
    "    out = xci.max_n_day_precipitation_amount(pr, freq=\"YS\")\n",
    "    out.attrs[\"units\"] = \"mm\"\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def rx5day(pr):\n",
    "    \"\"\"'Max 5-day precip' - the max 5-day precip value recorded for a year.\n",
    "    \n",
    "    Args:\n",
    "        pr (xarray.DataArray): daily total precip values\n",
    "        \n",
    "    Returns:\n",
    "        Max 5-day precip for each year\n",
    "    \"\"\"\n",
    "    out = xci.max_n_day_precipitation_amount(pr, 5, freq=\"YS\")\n",
    "    out.attrs[\"units\"] = \"mm\"\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def su(tasmax):\n",
    "    \"\"\"'Summer days' - the number of days with tasmax above 25 C\n",
    "    \n",
    "    Args:\n",
    "        tasmax (xarray.DataArray): daily maximum temperature values for a year\n",
    "        \n",
    "    Returns:\n",
    "        Number of summer days for each year\n",
    "    \"\"\"\n",
    "    return xci.tx_days_above(tasmax, \"25 degC\", freq=\"YS\")\n",
    "\n",
    "\n",
    "def dw(tasmin):\n",
    "    \"\"\"'Deep winter days' - the number of days with tasmin below -30 C\n",
    "    \n",
    "    Args:\n",
    "        tasmin (xarray.DataArray): daily maximum temperature values for a year\n",
    "        \n",
    "    Returns:\n",
    "        Number of deep winter days for each year\n",
    "    \"\"\"\n",
    "    return xci.tn_days_below(tasmin, thresh=\"-30 degC\", freq=\"YS\")\n",
    "\n",
    "\n",
    "def wsdi(tasmax, hist_da):\n",
    "    \"\"\"'Warm spell duration index' - Annual count of occurrences of at least 5 consecutive days with daily max T above 90th percentile of historical values for the date\n",
    "    \n",
    "    Args:\n",
    "        tasmax (xarray.DataArray): daily maximum temperature values\n",
    "        hist_da (xarray.DataArray): historical daily maximum temperature values\n",
    "        \n",
    "    Returns:\n",
    "        Warm spell duration index for each year\n",
    "    \"\"\"\n",
    "    tasmax_per = percentile_doy(hist_da, per=90).sel(percentiles=90)\n",
    "    return xci.warm_spell_duration_index(tasmax, tasmax_per, window=6, freq=\"YS\").drop(\"percentiles\")\n",
    "\n",
    "\n",
    "def csdi(tasmin, hist_da):\n",
    "    \"\"\"'Cold spell duration index' - Annual count of occurrences of at least 5 consecutive days with daily min T below 10th percentile of historical values for the date\n",
    "    \n",
    "    Args:\n",
    "        tasmin (xarray.DataArray): daily minimum temperature values for a year\n",
    "        hist_da (xarray.DataArray): historical daily minimum temperature values\n",
    "        \n",
    "    Returns:\n",
    "        Cold spell duration index for each year\n",
    "    \"\"\"\n",
    "    tasmin_per = percentile_doy(hist_da, per=10).sel(percentiles=10)\n",
    "    return xci.cold_spell_duration_index(tasmin, tasmin_per, window=6, freq=\"YS\").drop(\"percentiles\")\n",
    "\n",
    "\n",
    "def r10mm(pr):\n",
    "    \"\"\"'Heavy precip days' - number of days in a year with over 10mm of precip\n",
    "    \n",
    "    Args:\n",
    "        pr (xarray.DataArray): daily total precip values\n",
    "        \n",
    "    Returns:\n",
    "        Number of heavy precip days for each year\n",
    "    \"\"\"\n",
    "    # code based on xclim.indices._threshold.tg_days_above\n",
    "    thresh = \"10 mm/day\"\n",
    "    thresh = convert_units_to(thresh, pr)\n",
    "    f = threshold_count(pr, \">\", thresh, freq=\"YS\")\n",
    "    return to_agg_units(f, pr, \"count\")\n",
    "\n",
    "\n",
    "def cwd(pr):\n",
    "    \"\"\"'Consecutive wet days' - number of the most consecutive days with precip > 1 mm\n",
    "    \n",
    "    Args:\n",
    "        pr (xarray.DataArray): daily total precip values\n",
    "        \n",
    "    Returns:\n",
    "        Max number of consecutive wet days for each year\n",
    "    \"\"\"\n",
    "    return xci.maximum_consecutive_wet_days(pr, thresh=f\"1 mm/day\", freq=\"YS\")\n",
    "\n",
    "\n",
    "def cdd(pr):\n",
    "    \"\"\"'Consecutive dry days' - number of the most consecutive days with precip < 1 mm\n",
    "    \n",
    "    Args:\n",
    "        pr (xarray.DataArray): daily total precip values\n",
    "        \n",
    "    Returns:\n",
    "        Max number of consecutive dry days for each year\n",
    "    \"\"\"\n",
    "    return xci.maximum_consecutive_dry_days(pr, thresh=f\"1 mm/day\", freq=\"YS\")\n",
    "\n",
    "\n",
    "def compute_indicator(da, indicator, scenario, model, kwargs={}):\n",
    "    \"\"\"Summarize a DataArray according to a specified index / aggregation function\n",
    "    \n",
    "    Args:\n",
    "        da (xarray.DataArray): the DataArray object containing the base variable data to be summarized according to aggr\n",
    "        indicator (str): String corresponding to the name of the indicator to compute (assumes value is equal to the name of the corresponding function)\n",
    "        scenario (str): scenario being run (for new coordinate dimension)\n",
    "        model (str): model being run (for new coordinate dimension)\n",
    "        kwargs (dict): additional arguments for the index function being called\n",
    "            \n",
    "    Returns:\n",
    "        A new data array with dimensions year, latitude, longitude, in that order containing the summarized information\n",
    "    \"\"\"\n",
    "    new_da = globals()[indicator](da, **kwargs)\n",
    "    new_da = new_da.compute()\n",
    "    new_da.name = indicator  \n",
    "\n",
    "    # add model and scenario coordinate dimensions to the data array\n",
    "    coords_di = {\n",
    "        \"model\": model,\n",
    "        \"scenario\": scenario,\n",
    "    }\n",
    "\n",
    "    new_dims = list(coords_di.keys())\n",
    "    new_da = new_da.assign_coords(coords_di).expand_dims(new_dims)\n",
    "    # convert the time dimension to integer years instead of CF time objects\n",
    "    new_da = new_da.rename({\"time\": \"year\"}).assign_coords({\"year\": new_da.time.dt.year.values})\n",
    "\n",
    "    return new_da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0bd2eb-751a-4f7b-99e4-c810b6cbd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "# project\n",
    "\n",
    "# import indices\n",
    "# ignore all-nan slice warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "import time\n",
    "# tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ffef65ac-c17e-476e-97c9-21605953e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "\n",
    "\"\"\"Config file for setting shared paths, imports, etc across the project\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "out_dir = Path(\"/atlas_scratch/kmredilla/\")\n",
    "# path to directory containing CORDEX data\n",
    "ncar_dir = Path(\"/atlas_scratch/cparr4/ncar_replacement_data\")\n",
    "\n",
    "# path to dataset of extreme variables calculated on an annal basis\n",
    "#  for the entire domain of the CORDEX data.\n",
    "indicators_fp = out_dir.joinpath(\"ncar12km_indicators.nc\")\n",
    "\n",
    "# models, scenarios, and base variable names as found in the base CORDEX data\n",
    "models = [\n",
    "    # \"ACCESS1-3\",\n",
    "    # \"CanESM2\",\n",
    "    \"CCSM4\",\n",
    "    # \"CSIRO-Mk3-6-0\",\n",
    "    # \"GFDL-ESM2M\",\n",
    "    # \"HadGEM2-ES\",\n",
    "    # \"inmcm4\",\n",
    "    # \"MIROC5\",\n",
    "    \"MRI-CGCM3\"\n",
    "]\n",
    "\n",
    "scenarios = [\"rcp45\", \"rcp85\"]\n",
    "\n",
    "varnames = [\"pcp\", \"tmin\", \"tmax\"]\n",
    "\n",
    "# map from model variable names to possible index variable names\n",
    "indicator_varname_lu = {\n",
    "    'rx1day': 'pcp',\n",
    "    'rx5day': 'pcp',\n",
    "    'r10mm': 'pcp',\n",
    "    'cwd': 'pcp',\n",
    "    'cdd': 'pcp',\n",
    "    'hd': 'tmax',\n",
    "    'su': 'tmax',\n",
    "    'wsdi': 'tmax',\n",
    "    'cd': 'tmin',\n",
    "    'dw': 'tmin',\n",
    "    'csdi': 'tmin'\n",
    "}\n",
    "\n",
    "indicators = list(indicator_varname_lu.keys())\n",
    "\n",
    "# template filename\n",
    "temp_fn = \"{}_{}_BCSD_met_{}.nc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a20aae-e970-4d39-bd3c-efd5ecbc60ac",
   "metadata": {},
   "source": [
    "We will use functions from the `indicators.py` script to compute the indicators. Define a wrapper function for the `compute_indicator` function that will open the connection to a dataset, which is a collection of files from 2006-2100 for a particular model, and compute all indicators for that particular file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c31561dc-48ca-4c68-8015-593ec9126f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_compute_indicators(fps, scenario, model):\n",
    "    \"\"\"Read in data and compute all requested indices for a particular model variable, scenario, and model.\n",
    "    \n",
    "    Args:\n",
    "        fps (list): list of paths to the yearly met files for computing indicators from\n",
    "        scenario (str): scenario being run\n",
    "        model (str): model being run\n",
    "        \n",
    "    Returns:\n",
    "        summary_das (tuple): list of DataArrays containing indicator values (one for each indicator)\n",
    "    \"\"\"\n",
    "    # Should be ~6 GB to load ~100 years of data\n",
    "    # since each subdataset will need to be operated on multiple times, just load this into memory\n",
    "    ds = xr.open_mfdataset(fps)\n",
    "    ds = ds.load()\n",
    "    print(f\"data for {scenario}, {model} loaded into memory\")\n",
    "    # need to remove underscore in units for xclim :/\n",
    "    ds[\"tmin\"].attrs[\"units\"] = \"degC\"\n",
    "    ds[\"tmax\"].attrs[\"units\"] = \"degC\"\n",
    "        \n",
    "    summary_das = []\n",
    "    for indicator in indicators:\n",
    "        varname = indicator_varname_lu[indicator]\n",
    "        if indicator in [\"wsdi\", \"csdi\"]:\n",
    "            kwargs = {\"hist_da\": daymet_ds[varname]}\n",
    "            summary_das.append(compute_indicator(ds[varname], indicator, scenario, model, kwargs))\n",
    "        else:\n",
    "            summary_das.append(compute_indicator(ds[varname], indicator, scenario, model))\n",
    "\n",
    "        print(indicator, \"done\", end=\", \")\n",
    "    print()\n",
    "    return summary_das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf53c3-de3f-47a2-9b69-b27bba820d4c",
   "metadata": {},
   "source": [
    "Create global daymet dataset for use in computing WSDI and CSDI indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "23c6b0b0-be68-4076-b39f-5f67284a3ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.6 s, sys: 7.99 s, total: 47.6 s\n",
      "Wall time: 40.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daymet_ds = xr.open_mfdataset([f\"/atlas_scratch/cparr4/ncar_replacement_data/daymet/daymet_met_{year}.nc\" for year in range(1980, 2010)])\n",
    "daymet_ds = daymet_ds.load()\n",
    "# drop underscore from units in tmin/tmax, for xclim to be happy\n",
    "daymet_ds[\"tmin\"].attrs[\"units\"] = \"degC\"\n",
    "daymet_ds[\"tmax\"].attrs[\"units\"] = \"degC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c2f45-e365-4e66-9ae5-117c64c1b8be",
   "metadata": {},
   "source": [
    "Iterate over the projected models and scenarios and compute the indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ee4c95c-4658-4c5e-8bc2-579a32fa343d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data for rcp45, CCSM4 loaded into memory\n",
      "rx1day  done, rx5day  done, r10mm  done, cwd  done, cdd  done, hd  done, su  done, wsdi  done, cd  done, dw  done, csdi  done, \n",
      "data for rcp45, MRI-CGCM3 loaded into memory\n",
      "rx1day  done, rx5day  done, r10mm  done, cwd  done, cdd  done, hd  done, su  done, wsdi  done, cd  done, dw  done, csdi  done, \n",
      "data for rcp85, CCSM4 loaded into memory\n",
      "rx1day  done, rx5day  done, r10mm  done, cwd  done, cdd  done, hd  done, su  done, wsdi  done, cd  done, dw  done, csdi  done, \n",
      "data for rcp85, MRI-CGCM3 loaded into memory\n",
      "rx1day  done, rx5day  done, r10mm  done, cwd  done, cdd  done, hd  done, su  done, wsdi  done, cd  done, dw  done, csdi  done, \n",
      "CPU times: user 1h 5min 40s, sys: 15min 33s, total: 1h 21min 13s\n",
      "Wall time: 1h 19min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "for scenario in scenarios:\n",
    "    for model in models:\n",
    "        fps = [ncar_dir.joinpath(f\"{model}_{scenario}_BCSD_met_{year}.nc\") for year in range(2006, 2100)]\n",
    "        results.append(run_compute_indicators(fps, scenario, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ef266-0f7e-424f-abad-41ec0781a11f",
   "metadata": {},
   "source": [
    "Merge the individual DataArrays into a single dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7b49af67-482d-4620-abfd-8a425e038885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.1 s, sys: 5.23 s, total: 23.4 s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%time proj_indicators_ds = xr.merge([da for da_list in results for da in da_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa39258-6e34-4943-9e65-f8b329ed80c2",
   "metadata": {},
   "source": [
    "Process applicable indicators for the historical era (using Daymet dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6a6c52f7-922e-487f-90ec-e6cbf1a57e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rx1day done, rx5day done, r10mm done, cwd done, cdd done, hd done, su done, cd done, dw done, CPU times: user 1min 48s, sys: 32.6 s, total: 2min 21s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "summary_das = []\n",
    "for indicator in [\"rx1day\", \"rx5day\", \"r10mm\", \"cwd\", \"cdd\", \"hd\", \"su\", \"cd\", \"dw\"]:\n",
    "    varname = indicator_varname_lu[indicator]\n",
    "    summary_das.append(compute_indicator(daymet_ds[varname], indicator, \"historical\", \"daymet\"))\n",
    "    print(indicator, \"done\", end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8d1ea-c859-4165-833a-4bd78cfa5870",
   "metadata": {},
   "source": [
    "And combine into a Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "72ad03ab-06f4-4f58-acc8-c70b2834abf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 ms, sys: 21 µs, total: 20.7 ms\n",
      "Wall time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "%time daymet_indicators_ds = xr.merge(summary_das)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4f674-d483-490d-80ca-3bb4d182fc33",
   "metadata": {},
   "source": [
    "Convert 0's (which are null values from some of the xclim indicators) to -9999. Do this for projected indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2eb10c42-9dd0-4fe9-aef6-a6363c2bdae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan(da):\n",
    "    da.values[nan_mask] = -9999\n",
    "    da.attrs[\"_FillValue\"] = -9999\n",
    "    return da\n",
    "\n",
    "nan_mask = np.isnan(proj_indicators_ds[\"rx1day\"])\n",
    "\n",
    "for indicator in [\"r10mm\", \"wsdi\", \"csdi\", \"cwd\", \"cdd\", \"su\", \"dw\"]:\n",
    "    proj_indicators_ds[indicator] = replace_nan(proj_indicators_ds[indicator]).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46ad8f-68d3-4772-95b0-35af9771ea43",
   "metadata": {},
   "source": [
    "Then daymet indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5b28d0ce-3a8a-4aa2-83f6-644f74580346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting different nanmask because different data cubes\n",
    "nan_mask = np.isnan(daymet_indicators_ds[\"rx1day\"])\n",
    "\n",
    "for indicator in [\"r10mm\", \"cwd\", \"cdd\", \"su\", \"dw\"]:\n",
    "    # um this array isn't writeable? never seen this before\n",
    "    daymet_indicators_ds[indicator].values.setflags(write=1)\n",
    "    daymet_indicators_ds[indicator] = replace_nan(daymet_indicators_ds[indicator]).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27cbe7c-817a-450f-9381-154f6d73ca37",
   "metadata": {},
   "source": [
    "Then combine the projected and Daymet indicators Datasets together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5bc3e3ca-db1c-41f5-88eb-69238a2e796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_ds = xr.merge([daymet_indicators_ds, proj_indicators_ds])\n",
    "del indicators_ds.attrs[\"units\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c335ca5-5de9-47a2-af5d-ae5b63db6c60",
   "metadata": {},
   "source": [
    "Round certain indicators to reasonable precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ccff33f5-791f-4eaf-adb4-ee6e7b69b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for indicator in [\"hd\", \"cd\", \"rx1day\", \"rx5day\"]:\n",
    "    indicators_ds[indicator] = np.round(indicators_ds[indicator], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5837ba-a04d-4ad1-94e9-414e2edda14f",
   "metadata": {},
   "source": [
    "Add global metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b15f8148-9c23-48be-a6c7-035fe0dbc260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "indicators_ds.attrs = {\n",
    "    \"creation_date\": datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489720e-4ae8-4c41-a0d8-f2b60b31d1f8",
   "metadata": {},
   "source": [
    "Write to disk (might take a couple of minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b8dff980-e710-48cb-a9b6-4e701fe242d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88.6 ms, sys: 4.26 s, total: 4.35 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%time indicators_ds.to_netcdf(indicators_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07042cd-12bb-4109-a7a0-c02527baa994",
   "metadata": {},
   "source": [
    "done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
