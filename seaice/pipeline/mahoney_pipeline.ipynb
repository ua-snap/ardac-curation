{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd00eeb5",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The purpose of this notebook is to create a unified dataset of landfast ice extent for the Alaskan coastline from the Mahoney / InteRFACE data collection as described in the exploratory data analysis notebooks and summary documentation thereof.\n",
    "\n",
    "The flow here is as follows:\n",
    "\n",
    " 1. Create a DateTime index that encompasses the entire range of the data and gracefully handle that the atomic units of data here have variable ranges or \"durations\" on the order of 20 days. Also check that the turnover between calendar years is handled.\n",
    " 2. Map the existing data collection to this index.\n",
    " 3. For each item in the existing data collection constrain the valid array values and establish a common and well known spatial reference.\n",
    " 4. Create a merged raster from Chukchi and Beaufort data (or lackthereof) for each date in the time index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3577e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.merge import merge\n",
    "from pandas import Timestamp\n",
    "NCORES = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481db72",
   "metadata": {},
   "source": [
    "The inital step is to copy data from the source (SRC) to a pre-processing (DST) directory. We'll also set a destination directory for the ultimate processed data (MRG). Based on our exploratory work, we know that we just want the `slie` data and so we'll copy these data to our pre-processing directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f97e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SRC\"] = \"/atlas_scratch/kmredilla/ardac/landfast_seaice/\"\n",
    "src_p = Path(os.environ[\"SRC\"])\n",
    "\n",
    "os.environ[\"DST\"] = \"/atlas_scratch/cparr4/landfast_seaice/mahoney_preprocess\"\n",
    "dst_p = Path(os.environ[\"DST\"]).mkdir(parents=True, exist_ok=True)\n",
    "dst_p = Path(os.environ[\"DST\"])\n",
    "\n",
    "os.environ[\"MRG\"] = \"/atlas_scratch/cparr4/landfast_seaice/mahoney_preprocess/merged\"\n",
    "mrg_p = Path(os.environ[\"MRG\"]).mkdir(parents=True, exist_ok=True)\n",
    "mrg_p = Path(os.environ[\"MRG\"])\n",
    "\n",
    "slie_fps = [x for x in src_p.rglob(\"*_slie.tif\")]\n",
    "new_fps = [dst_p / ''.join(x.parent.parent.name.lower() + \"_\" + x.name) for x in slie_fps]\n",
    "\n",
    "# uncomment the two lines below to actually copy the data over\n",
    "# for src, dst in zip(slie_fps, new_fps):\n",
    "#     shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ac91d",
   "metadata": {},
   "source": [
    "Next we'll write a few functions to parse file names and convert day-of-year (DOY) style dates to a more readable and DateTime friendly YYYY-MM-DD format. The file names are of the style `chukchi_r2007_326-350_slie.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ff66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doy(fp):\n",
    "    \"\"\"Fetch the DOY range from a file name.\"\"\"\n",
    "    try:\n",
    "        doy = re.match(r'.*([0-3][0-9][0-9]-[0-3][0-9][0-9])', fp).group(1)\n",
    "    except:\n",
    "        doy = re.match(r'.*([0-3][0-9][0-9]_[0-3][0-9][0-9])', fp).group(1)\n",
    "        doy = doy.replace(\"_\", \"-\")\n",
    "    return doy\n",
    "\n",
    "\n",
    "def get_re_year(fp):\n",
    "    \"\"\"Fetch a single year (YYYY) from a file name.\"\"\"\n",
    "    year = re.match(r'.*([1-3][0-9]{3})', fp).group(1)\n",
    "    return int(year)\n",
    "\n",
    "\n",
    "def split_doy(doy_range):\n",
    "    \"\"\"Split a DOY range string into a integer start and end.\"\"\"\n",
    "    doy_start, doy_end = doy_range.split(\"-\")\n",
    "    return int(doy_start), int(doy_end)    \n",
    "\n",
    "\n",
    "def doy_date_to_YYYYMMDD(year, days):\n",
    "    \"\"\"Convert a a DOY + Year date to a YYYY-MM-DD datetime object.\"\"\"\n",
    "    dt = datetime.datetime(year, 1, 1) + datetime.timedelta(days - 1)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e607afe",
   "metadata": {},
   "source": [
    "Next we'll construct a lookup where we can see all the date information associated with each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "113590f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {}\n",
    "for fp in new_fps:\n",
    "    k = fp.name\n",
    "    di[k] = {}\n",
    "    di[k][\"start_year\"] = get_re_year(k)\n",
    "    di[k][\"doy_range\"] = get_doy(k)\n",
    "    di[k][\"doy_start\"], di[k][\"doy_end\"] = split_doy(di[k][\"doy_range\"])\n",
    "    di[k][\"dt_start\"] = doy_date_to_YYYYMMDD(di[k][\"start_year\"], di[k][\"doy_start\"])\n",
    "    di[k][\"dt_end\"] = doy_date_to_YYYYMMDD(di[k][\"start_year\"], di[k][\"doy_end\"])\n",
    "    di[k][\"dt_duration\"] = di[k][\"dt_end\"] - di[k][\"dt_start\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e220bd4",
   "metadata": {},
   "source": [
    "But we need to be aware that negative durations are lurking where we jump to the next calendar year, like in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa8fe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_year': 2000,\n",
       " 'doy_range': '349-003',\n",
       " 'doy_start': 349,\n",
       " 'doy_end': 3,\n",
       " 'dt_start': datetime.datetime(2000, 12, 14, 0, 0),\n",
       " 'dt_end': datetime.datetime(2000, 1, 3, 0, 0),\n",
       " 'dt_duration': datetime.timedelta(days=-346)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di['beaufort_r2000_349-003_slie.tif']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96e2a7",
   "metadata": {},
   "source": [
    "We'll create a function to check for these instances and then bump the year as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf8f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_end_year(di):\n",
    "    \"\"\"Adds an end year for each file and handles rollover cases.\"\"\"\n",
    "    for k in di.keys():\n",
    "        if di[k][\"dt_duration\"].days < 0:\n",
    "            di[k][\"end_year\"] = di[k][\"start_year\"] + 1\n",
    "            di[k][\"dt_end\"] = doy_date_to_YYYYMMDD(di[k][\"end_year\"], di[k][\"doy_end\"])\n",
    "            di[k][\"dt_duration\"] = di[k][\"dt_end\"] - di[k][\"dt_start\"]\n",
    "\n",
    "        else:\n",
    "            di[k][\"end_year\"] = di[k][\"start_year\"]\n",
    "\n",
    "\n",
    "set_end_year(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece1b09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_year': 2000,\n",
       " 'doy_range': '349-003',\n",
       " 'doy_start': 349,\n",
       " 'doy_end': 3,\n",
       " 'dt_start': datetime.datetime(2000, 12, 14, 0, 0),\n",
       " 'dt_end': datetime.datetime(2001, 1, 3, 0, 0),\n",
       " 'dt_duration': datetime.timedelta(days=20),\n",
       " 'end_year': 2001}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di['beaufort_r2000_349-003_slie.tif']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406d059",
   "metadata": {},
   "source": [
    "Now that we have all the date information we can establish the start and end dates of the entire collection and create a a DateTime index that encompasses all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71bd60c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996-10-17 00:00:00\n",
      "2008-07-14 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['1996-10-17', '1996-10-18', '1996-10-19', '1996-10-20',\n",
       "               '1996-10-21', '1996-10-22', '1996-10-23', '1996-10-24',\n",
       "               '1996-10-25', '1996-10-26',\n",
       "               ...\n",
       "               '2008-07-05', '2008-07-06', '2008-07-07', '2008-07-08',\n",
       "               '2008-07-09', '2008-07-10', '2008-07-11', '2008-07-12',\n",
       "               '2008-07-13', '2008-07-14'],\n",
       "              dtype='datetime64[ns]', length=4289, freq='D')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_first_dt(di):\n",
    "    \"\"\"Get the earliest chronological start date from the lookup.\"\"\"\n",
    "    start_dts = []\n",
    "    for k in di.keys():\n",
    "        start_dts.append(di[k][\"dt_start\"])\n",
    "    start_dt = sorted(start_dts)[0]\n",
    "    return start_dt\n",
    "\n",
    "\n",
    "def get_last_dt(di):\n",
    "    \"\"\"Get the latest chronological end date from the lookup.\"\"\"\n",
    "    end_dts = []\n",
    "    for k in di.keys():\n",
    "        end_dts.append(di[k][\"dt_end\"])\n",
    "    last_dt = sorted(end_dts)[-1]\n",
    "    return last_dt\n",
    "\n",
    "end = get_last_dt(di)\n",
    "start = get_first_dt(di)\n",
    "dt_range = pd.date_range(start=start, end=end)\n",
    "\n",
    "print(start)\n",
    "print(end)\n",
    "dt_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5967da",
   "metadata": {},
   "source": [
    "Because each file spans many days, a single calendar day can be represented by more than one raster. We need to map date to the representative rasters by checking if the date is in the range represented by the file. We'll want to know how many files match from the Beaufort region, how many from the Chukchi region, and how many match total. We also expect that some calendar days will have no matches at all (e.g., August, which is not in the seasonal ice cycle). We'll create a new lookup keyed by each date in the DateTime Index that stores the above information and the matching file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd83df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_in_range(start, end, x):\n",
    "    \"\"\"Return true if x is in the range [start, end]\"\"\"\n",
    "    if start <= end:\n",
    "        return start <= x <= end\n",
    "    else:\n",
    "        return start <= x or x <= end\n",
    "\n",
    "dt_di = {}\n",
    "\n",
    "for dt in dt_range:\n",
    "    dt_di[dt] = {}\n",
    "    dt_di[dt][\"matching data\"] = []\n",
    "    beaufort_count = 0\n",
    "    chukchi_count = 0\n",
    "    \n",
    "    for k in di.keys():\n",
    "        if time_in_range(di[k][\"dt_start\"], di[k][\"dt_end\"], dt):\n",
    "            dt_di[dt][\"matching data\"].append(k)\n",
    "            \n",
    "            if \"beaufort\" in k.lower():\n",
    "                beaufort_count += 1\n",
    "            if \"chukchi\" in k.lower():\n",
    "                chukchi_count += 1\n",
    "            \n",
    "    if len(dt_di[dt][\"matching data\"]) == 0:\n",
    "        dt_di[dt][\"matching data\"].append(\"no data\")\n",
    "    \n",
    "    dt_di[dt][\"beaufort_count\"] = beaufort_count\n",
    "    dt_di[dt][\"chukchi_count\"] = chukchi_count\n",
    "    dt_di[dt][\"match_count\"] = chukchi_count + beaufort_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55392148",
   "metadata": {},
   "source": [
    "We can see how matching files are distributed amongst the days by flipping this to a DataFrame. This isn' strictly necessary, but it is convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7334dd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matching data</th>\n",
       "      <th>beaufort_count</th>\n",
       "      <th>chukchi_count</th>\n",
       "      <th>match_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-02-16</th>\n",
       "      <td>[beaufort_r2007_029-048_slie.tif, beaufort_r20...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-23</th>\n",
       "      <td>[beaufort_r2006_058-084_slie.tif, beaufort_r20...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-06-19</th>\n",
       "      <td>[beaufort_r2000_147-171_slie.tif, beaufort_r20...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-18</th>\n",
       "      <td>[beaufort_r2006_049-077_slie.tif, beaufort_r20...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-17</th>\n",
       "      <td>[beaufort_r2006_049-077_slie.tif, beaufort_r20...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                matching data beaufort_count  \\\n",
       "2007-02-16  [beaufort_r2007_029-048_slie.tif, beaufort_r20...              3   \n",
       "2006-03-23  [beaufort_r2006_058-084_slie.tif, beaufort_r20...              3   \n",
       "2000-06-19  [beaufort_r2000_147-171_slie.tif, beaufort_r20...              3   \n",
       "2006-03-18  [beaufort_r2006_049-077_slie.tif, beaufort_r20...              3   \n",
       "2006-03-17  [beaufort_r2006_049-077_slie.tif, beaufort_r20...              3   \n",
       "\n",
       "           chukchi_count match_count  \n",
       "2007-02-16             3           6  \n",
       "2006-03-23             3           6  \n",
       "2000-06-19             3           6  \n",
       "2006-03-18             3           6  \n",
       "2006-03-17             3           6  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dt_di).T\n",
    "df.sort_values(\"match_count\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2107a0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matching data</th>\n",
       "      <th>beaufort_count</th>\n",
       "      <th>chukchi_count</th>\n",
       "      <th>match_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-08-31</th>\n",
       "      <td>[no data]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-08-09</th>\n",
       "      <td>[no data]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-08-08</th>\n",
       "      <td>[no data]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-08-07</th>\n",
       "      <td>[no data]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-08-06</th>\n",
       "      <td>[no data]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           matching data beaufort_count chukchi_count match_count\n",
       "2002-08-31     [no data]              0             0           0\n",
       "2002-08-09     [no data]              0             0           0\n",
       "2002-08-08     [no data]              0             0           0\n",
       "2002-08-07     [no data]              0             0           0\n",
       "2002-08-06     [no data]              0             0           0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"match_count\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d6e3f",
   "metadata": {},
   "source": [
    "Some individual days are represented by as many as six rasters across both regions! We'll return to this DataFrame to handle how to unify the Chukchi and Beaufort regions based on how many rasters from each are represent each calendar date, if any.\n",
    "\n",
    "The next step is to constrain our array values to a known range. We know from our EDA work the value schema in the existing collection is as follows:\n",
    "\n",
    " - 0: No landfast sea ice is present for this pixel. This means either water, or sea-ice that is NOT landfast.\n",
    " - 255: landfast sea ice is present for this pixel\n",
    " - 128: A landmask.\n",
    " - 63, 64, other oddball values: NoData\n",
    "\n",
    "These values can be reduced to binary set where `0` indicates the absence of landfast sea ice and `1` the presence of it.\n",
    "\n",
    "We'll define a function that forces this array value mapping and writes a new GeoTIFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "982990b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrain_arr_values(args):\n",
    "    \n",
    "    \"\"\"Writes a GeoTIFF of conditionally set 0 or 1 values\"\"\"\n",
    "    \n",
    "    fp, profile, out_fp = args\n",
    "    with rio.open(fp) as src:\n",
    "        arr = src.read(1)\n",
    "    \n",
    "    arr[arr != 255] = 0\n",
    "    arr[arr == 255] = 1\n",
    "    \n",
    "    with rio.open(out_fp, 'w', **profile) as dst:\n",
    "        dst.write(arr, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545dd6e",
   "metadata": {},
   "source": [
    "At this stage we can also create cohesive spatial references and raster creation profiles. Ultimately we want to create a merged raster as well, so we'll create three new rasters and raster profiles:\n",
    "1. An empty mask (all zeros) for the Chukchi Region\n",
    "2. An empty mask (all zeros) for the Beaufort Region\n",
    "3. An empty mask (all zeros) for the merged region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb3752b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "beauf_sample = new_fps[0]\n",
    "chuk_sample = new_fps[-1]\n",
    "\n",
    "with rio.open(beauf_sample) as beauf_src:\n",
    "    beauf_src = rio.open(beauf_sample)\n",
    "    beauf_profile = beauf_src.profile\n",
    "beauf_profile[\"crs\"] = CRS.from_epsg(3338)\n",
    "beauf_profile.update(compress=\"lzw\")\n",
    "\n",
    "with rio.open(chuk_sample):\n",
    "    chuk_src = rio.open(chuk_sample)\n",
    "    chuk_profile = chuk_src.profile\n",
    "chuk_profile[\"crs\"] = CRS.from_epsg(3338)\n",
    "chuk_profile.update(compress=\"lzw\")\n",
    "\n",
    "arr_merge, aff_merge = merge([beauf_src, chuk_src])\n",
    "\n",
    "new_profile = chuk_profile.copy()\n",
    "new_profile[\"height\"], new_profile[\"width\"] = arr_merge[0].shape\n",
    "new_profile[\"transform\"] = aff_merge\n",
    "new_profile[\"crs\"] = CRS.from_epsg(3338)\n",
    "new_profile.update(compress=\"lzw\")\n",
    "\n",
    "with rio.open(mrg_p / \"both_region_mask.tif\", 'w', **new_profile) as dst:\n",
    "        dst.write(np.zeros(arr_merge[0].shape), 1)\n",
    "\n",
    "with rio.open(mrg_p / \"beaufort_mask.tif\", 'w', **beauf_profile) as dst:\n",
    "        dst.write(np.zeros((beauf_profile[\"height\"], beauf_profile[\"width\"])), 1)\n",
    "\n",
    "with rio.open(mrg_p / \"chukchi_mask.tif\", 'w', **chuk_profile) as dst:\n",
    "        dst.write(np.zeros((chuk_profile[\"height\"], chuk_profile[\"width\"])), 1)\n",
    "        \n",
    "# we want the mask paths for later\n",
    "both_mask_fp = mrg_p / \"both_region_mask.tif\"\n",
    "chuk_mask_fp = mrg_p / \"chukchi_mask.tif\"\n",
    "beauf_mask_fp = mrg_p / \"beaufort_mask.tif\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32437033",
   "metadata": {},
   "source": [
    "Now that we have established cohesive raster profiles, we can generate the function arguments that will be used to consolidate the array values. The fixed output GeoTIFFs will just have a \"arrfix\" prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4a25ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chuk_fps = list(dst_p.glob(\"chukchi*\"))\n",
    "beauf_fps = list(dst_p.glob(\"beaufort*\"))\n",
    "\n",
    "beauf_args = [\n",
    "    (\n",
    "        fp,\n",
    "        beauf_profile,\n",
    "        fp.parent / (\"arrfix_\" + fp.name)\n",
    "    ) \n",
    "    for fp in beauf_fps\n",
    "]\n",
    "\n",
    "chuk_args = [\n",
    "    (\n",
    "        fp,\n",
    "        chuk_profile,\n",
    "        fp.parent / (\"arrfix_\" + fp.name)\n",
    "    ) \n",
    "    for fp in chuk_fps\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e3abc",
   "metadata": {},
   "source": [
    "The following functions will execute the array correction for each region in parallel. This is a significant speedup. However, these functions can sometimes hang or \"deadlock\" at the end of their execution. Sometimes (not always), this hang results in a few of the fixed GeoTIFFs not being produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_beaufort():\n",
    "    with Pool(NCORES) as pool:\n",
    "        _ = [\n",
    "            result for result in tqdm(\n",
    "                pool.imap_unordered(constrain_arr_values, beauf_args), \n",
    "                total=len(beauf_args),\n",
    "                desc=f\"Fixing Beaufort array values and writing to new GeoTIFFs...\",\n",
    "            )\n",
    "        ]\n",
    "        pool.close()\n",
    "        pool.terminate()\n",
    "        \n",
    "do_beaufort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832881e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_chukchi():   \n",
    "    with Pool(NCORES) as pool:\n",
    "        _ = [\n",
    "            result for result in tqdm(\n",
    "                pool.imap_unordered(constrain_arr_values, chuk_args), \n",
    "                total=len(chuk_args),\n",
    "                desc=f\"Fixing Chukchi array values and writing to new GeoTIFFs...\",\n",
    "            )\n",
    "        ]\n",
    "        pool.close()\n",
    "        pool.terminate()\n",
    "do_chukchi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116d92a",
   "metadata": {},
   "source": [
    "Verify the correct number of GeoTIFFs was generated. If not, you may need to restart the kernel and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67a329b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(dst_p.glob(\"arrfix*\"))) == len(slie_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76df44",
   "metadata": {},
   "source": [
    "Now that we have constrained the data values and spatial reference and raster profiles, we can move to creating a daily GeoTIFF that includes all possible data. There are four possibles cases of data availability for each date in our DateTime Index.\n",
    "\n",
    "1. No data is available at all.\n",
    "2. A single raster exists from only one region.\n",
    "3. Multiple rasters exist from only one region.\n",
    "4. Multiple rasters exist from both regions.\n",
    "\n",
    "We can handle these cases like so:\n",
    "\n",
    "Case 1: Skip these dates for now, there is no data available. These dates can be brought back into a datacube at a later time if needed.\n",
    "\n",
    "Case 2: Take the single raster and merge it with the mask for whichever region has no data.\n",
    "\n",
    "Case 3: Take all the rasters and create a 3D array of values, and filter for the maximum of the time index. Then merge these data with the mask for whichever region has no data.\n",
    "\n",
    "Case 4. Same as above, but do the stacking and filtering for both regions, and then then merge those two outputs together.\n",
    "\n",
    "We can map these conditions (cases 1 through 4) to our DataFrame for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06cf1b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (df[\"match_count\"] == 0),\n",
    "    (df[\"match_count\"] == 1),\n",
    "    ((df[\"match_count\"] > 1) & (df[\"chukchi_count\"] * df[\"beaufort_count\"] == 0)),\n",
    "    ((df[\"match_count\"] > 1) & (df[\"chukchi_count\"] * df[\"beaufort_count\"] != 0))]\n",
    "choices = [1, 2, 3, 4]\n",
    "df[\"merge_case\"] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b63f5a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matching data</th>\n",
       "      <th>beaufort_count</th>\n",
       "      <th>chukchi_count</th>\n",
       "      <th>match_count</th>\n",
       "      <th>merge_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-07-09</th>\n",
       "      <td>[no data]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           matching data beaufort_count chukchi_count match_count  merge_case\n",
       "1997-07-09     [no data]              0             0           0           1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.merge_case == 1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8df6d064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matching data</th>\n",
       "      <th>beaufort_count</th>\n",
       "      <th>chukchi_count</th>\n",
       "      <th>match_count</th>\n",
       "      <th>merge_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-10-17</th>\n",
       "      <td>[chukchi_r1996_291-309_slie.tif]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               matching data beaufort_count chukchi_count  \\\n",
       "1996-10-17  [chukchi_r1996_291-309_slie.tif]              0             1   \n",
       "\n",
       "           match_count  merge_case  \n",
       "1996-10-17           1           2  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.merge_case == 2].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6904fc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matching data</th>\n",
       "      <th>beaufort_count</th>\n",
       "      <th>chukchi_count</th>\n",
       "      <th>match_count</th>\n",
       "      <th>merge_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-10-24</th>\n",
       "      <td>[chukchi_r1996_291-309_slie.tif, chukchi_r1996...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                matching data beaufort_count  \\\n",
       "1996-10-24  [chukchi_r1996_291-309_slie.tif, chukchi_r1996...              0   \n",
       "\n",
       "           chukchi_count match_count  merge_case  \n",
       "1996-10-24             2           2           3  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.merge_case == 3].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fbb847b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matching data</th>\n",
       "      <th>beaufort_count</th>\n",
       "      <th>chukchi_count</th>\n",
       "      <th>match_count</th>\n",
       "      <th>merge_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999-10-08</th>\n",
       "      <td>[beaufort_r1999_276-297_slie.tif, chukchi_r199...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                matching data beaufort_count  \\\n",
       "1999-10-08  [beaufort_r1999_276-297_slie.tif, chukchi_r199...              1   \n",
       "\n",
       "           chukchi_count match_count  merge_case  \n",
       "1999-10-08             1           2           4  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.merge_case == 4].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5f635",
   "metadata": {},
   "source": [
    "Now that we've defined how to handle each case, we'll create functions to execute that logic. When called, each function will create a single GeoTIFF that encompasses both regions. These functions are set up to take a TimeStamp object as the arguement and then query the lookup for data associated with that date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4f7978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_fixed_one_arr_one_region(k):\n",
    "    \"\"\"This function handles the array-merge case #2.\"\"\"\n",
    "    if dt_di[k][\"chukchi_count\"] > dt_di[k][\"beaufort_count\"]:\n",
    "        mask_fp = beauf_mask_fp\n",
    "    else:\n",
    "        mask_fp = chuk_mask_fp\n",
    "    \n",
    "    fp = dst_p / dt_di[k][\"matching data\"][0]\n",
    "    fixed_fp = fp.parent / (\"arrfix_\" + fp.name)\n",
    "    \n",
    "    data_src = rio.open(fixed_fp)\n",
    "    mask_src = rio.open(mask_fp)\n",
    "    out_arr, out_aff = merge([data_src, mask_src])\n",
    "    data_src.close()\n",
    "    mask_src.close()\n",
    "    \n",
    "    out_merge_fp = Path(mrg_p / (\"ak_landfast_ice\" + str(k).split(' ')[0] + \".tif\"))\n",
    "    with rio.open(out_merge_fp, 'w', **new_profile) as dst:\n",
    "        dst.write(out_arr[0], 1)\n",
    "\n",
    "\n",
    "def use_fixed_many_arrs_one_region(k):\n",
    "    \"\"\"This function handles the array-merge case #3.\"\"\"\n",
    "    if dt_di[k][\"chukchi_count\"] > dt_di[k][\"beaufort_count\"]:\n",
    "        mask_fp = beauf_mask_fp\n",
    "        data_profile = chuk_profile\n",
    "    else:\n",
    "        mask_fp = chuk_mask_fp\n",
    "        data_profile = beauf_profile\n",
    "        \n",
    "    data_arrs = []\n",
    "    for src in dt_di[k][\"matching data\"]:\n",
    "        fp = dst_p / src\n",
    "        fixed_fp = fp.parent / (\"arrfix_\" + fp.name)\n",
    "        \n",
    "        with rio.open(fixed_fp) as data_src:\n",
    "            data_arrs.append(data_src.read(1))\n",
    "        \n",
    "    stack = np.dstack(data_arrs)\n",
    "    time_max = np.nanmax(stack, axis=2)\n",
    "    time_max_fp = Path(mrg_p / (\"tmp_max_of_\" + str(k).split(' ')[0] + \".tif\"))\n",
    "    \n",
    "    with rio.open(time_max_fp, 'w', **data_profile) as dst:\n",
    "        dst.write(time_max, 1)\n",
    "        \n",
    "    time_max_data_src = rio.open(time_max_fp)\n",
    "    mask_src = rio.open(mask_fp)\n",
    "    out_arr, out_aff = merge([time_max_data_src, mask_src])\n",
    "    time_max_data_src.close()\n",
    "    mask_src.close()\n",
    "\n",
    "    out_merge_fp = Path(mrg_p / (\"ak_landfast_ice\" + str(k).split(' ')[0] + \".tif\")) \n",
    "    with rio.open(out_merge_fp, 'w', **new_profile) as dst:\n",
    "        dst.write(out_arr[0], 1)\n",
    " \n",
    "    \n",
    "def use_fixed_many_arrs_both_regions(k):\n",
    "    \"\"\"This function handles the array-merge case #4.\"\"\"\n",
    "    beauf_srcs = [x for x in dt_di[k][\"matching data\"] if \"beauf\" in x]\n",
    "    chuk_srcs = [x for x in dt_di[k][\"matching data\"] if \"chuk\" in x]\n",
    "    \n",
    "    if len(beauf_srcs) == 1:\n",
    "        fp = dst_p / beauf_srcs[0]\n",
    "        beauf_to_merge_src = fp.parent / (\"arrfix_\" + fp.name)\n",
    "    else:\n",
    "        data_arrs = []\n",
    "        for src in beauf_srcs:\n",
    "            fp = dst_p / src\n",
    "            fixed_fp = fp.parent / (\"arrfix_\" + fp.name)\n",
    "        \n",
    "        with rio.open(fixed_fp) as data_src:\n",
    "            data_arrs.append(data_src.read(1))\n",
    "        \n",
    "        stack = np.dstack(data_arrs)\n",
    "        time_max = np.nanmax(stack, axis=2)\n",
    "        beauf_to_merge_src = Path(mrg_p / (\"tmp_beauf_max_of_\" + str(k).split(' ')[0] + \".tif\"))\n",
    "    \n",
    "        with rio.open(beauf_to_merge_src, 'w', **beauf_profile) as dst:\n",
    "            dst.write(time_max, 1)\n",
    "\n",
    "    # do same code above for chukchi\n",
    "    if len(chuk_srcs) == 1:\n",
    "        fp = dst_p / chuk_srcs[0]\n",
    "        chuk_to_merge_src = fp.parent / (\"arrfix_\" + fp.name)\n",
    "    else:\n",
    "        data_arrs = []\n",
    "        for src in chuk_srcs:\n",
    "            fp = dst_p / src\n",
    "            fixed_fp = fp.parent / (\"arrfix_\" + fp.name)\n",
    "        \n",
    "        with rio.open(fixed_fp) as data_src:\n",
    "            data_arrs.append(data_src.read(1))\n",
    "        \n",
    "        stack = np.dstack(data_arrs)\n",
    "        time_max = np.nanmax(stack, axis=2)\n",
    "        chuk_to_merge_src = Path(mrg_p / (\"tmp_chuk_max_of_\" + str(k).split(' ')[0] + \".tif\"))\n",
    "    \n",
    "        with rio.open(chuk_to_merge_src, 'w', **chuk_profile) as dst:\n",
    "            dst.write(time_max, 1)\n",
    "\n",
    "    # then merge chukchi_to_merge and beauf_to_merge\n",
    "    with rio.open(chuk_to_merge_src) as a, rio.open(beauf_to_merge_src) as b:\n",
    "        out_arr, out_aff = merge([a, b])\n",
    "    \n",
    "    out_merge_fp = Path(mrg_p / (\"ak_landfast_ice\" + str(k).split(' ')[0] + \".tif\")) \n",
    "    with rio.open(out_merge_fp, 'w', **new_profile) as dst:\n",
    "        dst.write(out_arr[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5afcc4a",
   "metadata": {},
   "source": [
    "Now we'll generate the arguments (timestamps) for use in the the above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f982eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_case_2 = df[df.merge_case == 2]\n",
    "dt_arr_di_case2 = merge_case_2.T.to_dict()\n",
    "\n",
    "merge_case_3 = df[df.merge_case == 3]\n",
    "dt_arr_di_case3 = merge_case_3.T.to_dict()\n",
    "\n",
    "merge_case_4 = df[df.merge_case == 4]\n",
    "dt_arr_di_case4 = merge_case_4.T.to_dict()\n",
    "\n",
    "merge_case2_args = [k for k in dt_arr_di_case2.keys()]\n",
    "merge_case3_args = [k for k in dt_arr_di_case3.keys()]\n",
    "merge_case4_args = [k for k in dt_arr_di_case4.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a056a2",
   "metadata": {},
   "source": [
    "As before, these multiprocessing functions may hang toward the end. You may need to restart the kernel and try again. This is getting the job done at the moment, but it may be worth trying to use `concurrent.futures.ProcessPoolExecutor` instead or just upgrading to Python 3.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_merge_case_two():   \n",
    "    with Pool(NCORES) as pool:\n",
    "        _ = [\n",
    "            result for result in tqdm(\n",
    "                pool.imap_unordered(use_fixed_one_arr_one_region, merge_case2_args), \n",
    "                total=len(merge_case2_args),\n",
    "                desc=f\"Merge single raster/region (case 2) array values and writing to new GeoTIFFs...\",\n",
    "            )\n",
    "        ]\n",
    "        pool.close()\n",
    "        pool.terminate()\n",
    "do_merge_case_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_merge_case_three():   \n",
    "    with Pool(NCORES) as pool:\n",
    "        _ = [\n",
    "            result for result in tqdm(\n",
    "                pool.imap_unordered(use_fixed_many_arrs_one_region, merge_case3_args), \n",
    "                total=len(merge_case3_args),\n",
    "                desc=f\"Merge multiple raster / single region (case 3) array values and writing to new GeoTIFFs...\",\n",
    "            )\n",
    "        ]\n",
    "        pool.close()\n",
    "        pool.terminate()\n",
    "do_merge_case_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1946307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_merge_case_four():   \n",
    "    with Pool(NCORES) as pool:\n",
    "        _ = [\n",
    "            result for result in tqdm(\n",
    "                pool.imap_unordered(use_fixed_many_arrs_both_regions, merge_case4_args), \n",
    "                total=len(merge_case4_args),\n",
    "                desc=f\"Merge multiple rasters from both regions (case 4) and write to new GeoTIFFs...\",\n",
    "            )\n",
    "        ]\n",
    "        pool.close()\n",
    "        pool.terminate()\n",
    "do_merge_case_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a18ca",
   "metadata": {},
   "source": [
    "Verify that all data were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a17d6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3492"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.merge_case != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9cd3905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3492\r\n"
     ]
    }
   ],
   "source": [
    "!ls $MRG/ak_landfast* | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501bc5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
