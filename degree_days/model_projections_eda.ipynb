{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8f174d-3d64-49d3-bd88-1ff58b4a5bde",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (Model Runs)\n",
    "\n",
    "The purpose of this notebook is to perform exploratory data analysis (EDA) for the NCAR statistically downscaled (BCSD) Alaska Near Surface Meteorology Daily Averages dataset. The data are 12 km resolution for the period 1950–2099\n",
    "\n",
    "The goal of this EDA notebook is to execute some of the normal tasks (what is here? what is missing? etc.) and understand the structures and value ranges within the data.\n",
    "\n",
    "The source data are annual netCDF files (with a daily frequency time step) that contain the data for a single model and scenario. There are 10 models and 2 scenarios. Within each model-scenario combinationt he Alaska Near Surface Meteorology Daily Averages have files names like `ACCESS1-3_rcp45_BCSD_met_1958.nc` where the `met` tag indicates that these files contain the following climate variables:\n",
    "\n",
    "    tmax (Maximum Daily 2-m air temperature, degrees C)\n",
    "    tmin (Minimum Daily 2-m air temperature, degrees C)\n",
    "    pcp (Daily precipitation, mm per day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237895db-8c3e-43cd-bce2-e6dbaa9357b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import DATA_DIR, daymet_dir, models, scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986c67a7-8870-4868-9051-98647043e49d",
   "metadata": {},
   "source": [
    "First we will verify that the expected number of files exists. There are ten (10) models, two scenarios (2), and 1950-2099 (150) years worth of output. We should therefore have 10 * 2 * 150 total files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c2a97a-25c3-494e-8af6-f001ed436074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_total_files = 10 * 2 * 150\n",
    "expected_total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8889735b-5c8d-49ed-bb34-44032d9f02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_model_files = []\n",
    "for model in models:\n",
    "    model_path = DATA_DIR / model\n",
    "    input_data = [x for x in list(model_path.rglob(\"*.nc*\"))]\n",
    "    if len(input_data) != 300:\n",
    "        print(model)\n",
    "        print(len(input_data))\n",
    "    projected_model_files.extend(input_data)\n",
    "\n",
    "assert len(projected_model_files) == expected_total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f851502a-663a-490e-b3df-579091b2a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [x for x in range(1950, 2100)]\n",
    "\n",
    "model_di = {}\n",
    "for model in models:\n",
    "    model_di[model] = []\n",
    "    \n",
    "scenario_di = {}\n",
    "for scenario in scenarios:\n",
    "    scenario_di[scenario] = []\n",
    "\n",
    "year_di = {}\n",
    "for year in years:\n",
    "    year_di[year] = []\n",
    "\n",
    "for nc_file in projected_model_files:\n",
    "    \n",
    "    file_model = nc_file.name.split(\"_\")[0]\n",
    "    file_scenario = nc_file.name.split(\"_\")[1]\n",
    "    file_year = nc_file.name.split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "    model_di[file_model].append(nc_file)\n",
    "    scenario_di[file_scenario].append(nc_file)\n",
    "    year_di[int(file_year)].append(nc_file)\n",
    "\n",
    "# basically asserting that no matter how we group the data (by model, by year, etc.)\n",
    "# the number of files in each group is equal (no missing or duplicated data)\n",
    "\n",
    "assert set([len(model_di[x]) for x in model_di.keys()]) == set([300])\n",
    "\n",
    "assert set([len(scenario_di[x]) for x in scenario_di.keys()]) == set([1500])\n",
    "\n",
    "assert set([len(year_di[x]) for x in year_di.keys()]) == set([20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e03026e-d4fd-48b0-9b97-f044f6668136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning ACCESS1-3 files...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:02<00:00, 103.22it/s]\n",
      "Scanning CanESM2 files...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:03<00:00, 79.00it/s]\n",
      "Scanning CCSM4 files...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:04<00:00, 74.10it/s]\n",
      "Scanning CSIRO-Mk3-6-0 files...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:03<00:00, 81.69it/s]\n",
      "Scanning GFDL-ESM2M files...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:03<00:00, 84.33it/s]\n",
      "Scanning HadGEM2-ES files...:  15%|███████████████████▌                                                                                                              | 45/300 [00:00<00:02, 85.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HadGEM2-ES_rcp45_BCSD_met_2005.nc4 has unusual dimensions of [209, 299, 334]\n",
      "HadGEM2-ES_rcp45_BCSD_met_2005.nc4 has unusual coordinates of Coordinates:\n",
      "    latitude   (y, x) float64 ...\n",
      "    longitude  (y, x) float64 ...\n",
      "  * time       (time) datetime64[ns] 2005-01-01 2005-01-02 ... 2005-11-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning HadGEM2-ES files...:  63%|████████████████████████████████████████████████████████████████████████████████▊                                                | 188/300 [00:02<00:01, 70.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HadGEM2-ES_rcp85_BCSD_met_2005.nc4 has unusual dimensions of [209, 299, 334]\n",
      "HadGEM2-ES_rcp85_BCSD_met_2005.nc4 has unusual coordinates of Coordinates:\n",
      "    latitude   (y, x) float64 ...\n",
      "    longitude  (y, x) float64 ...\n",
      "  * time       (time) datetime64[ns] 2005-01-01 2005-01-02 ... 2005-11-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning HadGEM2-ES files...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:03<00:00, 75.64it/s]\n",
      "Scanning inmcm4 files...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:04<00:00, 63.28it/s]\n",
      "Scanning MIROC5 files...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:05<00:00, 59.01it/s]\n",
      "Scanning MPI-ESM-MR files...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:05<00:00, 57.97it/s]\n",
      "Scanning MRI-CGCM3 files...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:05<00:00, 59.91it/s]\n"
     ]
    }
   ],
   "source": [
    "normal_dim = [209, 299, 365]\n",
    "leap_dim = [209, 299, 366]\n",
    "\n",
    "with xr.open_dataset(DATA_DIR / \"CCSM4\" / \"rcp85\" / \"CCSM4_rcp85_BCSD_met_2005.nc4\") as ds:\n",
    "    met_ref_coords = ds.coords\n",
    "    \n",
    "unruly_files = []\n",
    "ds_dims = []\n",
    "ds_indices = []\n",
    "ds_coords = []\n",
    "\n",
    "for model in models:\n",
    "    for nc_file in tqdm(model_di[model], desc=f\"Scanning {model} files...\"):\n",
    "        with xr.open_dataset(nc_file) as ds:\n",
    "\n",
    "            # check data three dimensional (time)\n",
    "            dims = list(ds.dims.values())\n",
    "            if sorted(dims) == normal_dim or sorted(dims) == leap_dim:\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"{nc_file.name} has unusual dimensions of {dims}\")\n",
    "                unruly_files.append(nc_file)\n",
    "\n",
    "            # check daily frequency including leap years\n",
    "            if ds.coords[\"time\"].shape[0] == 365 or ds.coords[\"time\"].shape[0] == 366:\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"{nc_file.name} has unusual coordinates of {ds.coords}\")\n",
    "                unruly_files.append(nc_file)\n",
    "\n",
    "            ds_indices.append(ds.indexes)\n",
    "\n",
    "            # check expected variables exist in each file as a DataArray\n",
    "            data_vars = set(list(ds.data_vars.keys()))\n",
    "            ref_vars = set([\"tmin\", \"tmax\", \"pcp\"])\n",
    "            test_vars = set.intersection(data_vars, ref_vars)\n",
    "            if test_vars == ref_vars:\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"{nc_file.name} only has the following data variables: {data_vars}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df52020-dc89-42b9-b2d4-17c7cc3e7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have the same indices.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([PosixPath('/atlas_scratch/Base_Data/AK_NCAR_12km/met/HadGEM2-ES/rcp45/HadGEM2-ES_rcp45_BCSD_met_2005.nc4'),\n",
       "       PosixPath('/atlas_scratch/Base_Data/AK_NCAR_12km/met/HadGEM2-ES/rcp85/HadGEM2-ES_rcp85_BCSD_met_2005.nc4')],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(np.unique(ds_indices)) == 1:\n",
    "    print(\"All files have the same indices.\")\n",
    "else:\n",
    "    print(\"Some files have different indices.\")\n",
    "    print(np.unique(ds_indices))\n",
    "np.unique(unruly_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf6271-ad17-4226-83f5-6659eaa5806e",
   "metadata": {},
   "source": [
    "So two files don't have a full year's worth of data - they seem to be missing data for the month of December. These files are HadGEM2-ES_rcp45_BCSD_met_2005.nc and HadGEM2-ES_rcp85_BCSD_met_2005.nc.\n",
    "After contacting NCAR we learned that this model run just didn't quite complete, and there is no plan to re-run it.\n",
    "We will set this particular model (HadGEM2-ES) aside when processing. Aside from these two files the data seem homogeneous enough - the files have the expected variables and are structured with the same 209 X 299 spatial grid and with a daily time-step, including leap years. The next step is to start sampling for value ranges and nodata values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca66502-4e50-4062-94e8-e30c0ae9bbdf",
   "metadata": {},
   "source": [
    "Next we will check the value ranges for `tmin` and `tmax`. These data are in C (will be converted to F later). Here are some historical, actual record temperature bounds from Wikipedia:\n",
    " - highest is 100 °F (38 °C) in Fort Yukon\n",
    " - lowest is −80 °F (−62 °C) in Prospect Creek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d18d4eaf-e168-446e-87a4-dea7e9545214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a delayed function to compute stats for all variables for a single file\n",
    "@delayed\n",
    "def compute_stats_for_file(file):\n",
    "    with xr.open_dataset(file) as ds:\n",
    "        file_stats = {}\n",
    "        for variable in [\"tmin\", \"tmax\"]:\n",
    "            da = ds[variable].chunk()  # chunk the data for parallel processing\n",
    "            file_stats[variable] = {\"filename\": file.name,\n",
    "                                    \"min_vals\": float(da.min()),\n",
    "                                    \"max_vals\": float(da.max()),\n",
    "                                    \"nan_count\": float(da.isnull().sum()), # also checking for a constant count of no data cells while we are \"in here\"\n",
    "                                   }\n",
    "        return file_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a8117a-9bee-459b-8ca7-b0baf1d62f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling values from the ACCESS1-3 model...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 12378.30it/s]\n",
      "Sampling values from the CanESM2 model...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 11204.63it/s]\n",
      "Sampling values from the CCSM4 model...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 14674.63it/s]\n",
      "Sampling values from the CSIRO-Mk3-6-0 model...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 16462.02it/s]\n",
      "Sampling values from the GFDL-ESM2M model...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 15415.89it/s]\n",
      "Sampling values from the HadGEM2-ES model...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 13583.76it/s]\n",
      "Sampling values from the inmcm4 model...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 13778.32it/s]\n",
      "Sampling values from the MIROC5 model...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 11573.37it/s]\n",
      "Sampling values from the MPI-ESM-MR model...: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 16479.49it/s]\n",
      "Sampling values from the MRI-CGCM3 model...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 16527.54it/s]\n"
     ]
    }
   ],
   "source": [
    "summary_stat_di = {}\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    all_file_stats = []\n",
    "\n",
    "    for nc_file in tqdm(model_di[model], desc=f\"Sampling values from the {model} model...\"):\n",
    "        all_file_stats.append(compute_stats_for_file(nc_file))\n",
    "    stat_result = dask.compute(*all_file_stats)\n",
    "    init_df = pd.DataFrame.from_dict(stat_result).T\n",
    "\n",
    "    output_dfs = []\n",
    "\n",
    "    for idx in init_df.index:\n",
    "        row_dict = init_df.loc[idx].to_dict()\n",
    "        df = pd.DataFrame(row_dict).T\n",
    "        df[\"variable\"] = idx\n",
    "        output_dfs.append(df)\n",
    "    summary_stat_di[model] = pd.concat(output_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24b311f-3c87-4f61-bdc9-e6a4eed031aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value summary (all scenarios) for model ACCESS1-3\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -68.3      29.5  17607965.0\n",
      "tmin         -67.2      14.5  17607965.0\n",
      "Minimum value summary (all scenarios) for model CanESM2\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -70.9      28.5  17607965.0\n",
      "tmin         -70.6      14.2  17607965.0\n",
      "Minimum value summary (all scenarios) for model CCSM4\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -68.8      28.4  17607965.0\n",
      "tmin         -67.4      14.5  17607965.0\n",
      "Minimum value summary (all scenarios) for model CSIRO-Mk3-6-0\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -69.3      28.6  17607965.0\n",
      "tmin         -67.3      14.1  17607965.0\n",
      "Minimum value summary (all scenarios) for model GFDL-ESM2M\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -69.1      28.8  17607965.0\n",
      "tmin         -67.1      14.9  17607965.0\n",
      "Minimum value summary (all scenarios) for model HadGEM2-ES\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -69.1      28.0  16112494.0\n",
      "tmin         -68.2      15.6  16112494.0\n",
      "Minimum value summary (all scenarios) for model inmcm4\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -70.3      29.4  17607965.0\n",
      "tmin         -69.6      15.5  17607965.0\n",
      "Minimum value summary (all scenarios) for model MIROC5\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -70.5      28.1  17607965.0\n",
      "tmin         -67.3      15.4  17607965.0\n",
      "Minimum value summary (all scenarios) for model MPI-ESM-MR\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -67.0      29.2  17607965.0\n",
      "tmin         -66.8      14.5  17607965.0\n",
      "Minimum value summary (all scenarios) for model MRI-CGCM3\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -68.8      29.4  17607965.0\n",
      "tmin         -67.6      15.7  17607965.0\n"
     ]
    }
   ],
   "source": [
    "for k in summary_stat_di.keys():\n",
    "    print(f\"Minimum value summary (all scenarios) for model {k}\")\n",
    "    mindf = summary_stat_di[k].drop([\"filename\"], axis=1).groupby(\"variable\").min().round(1)\n",
    "    print(mindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23d6246b-c695-4503-b712-3ccde6fc6f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value summary (all scenarios) for model ACCESS1-3\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -45.0      49.0  17656206.0\n",
      "tmin         -46.0      38.7  17656206.0\n",
      "Maximum value summary (all scenarios) for model CanESM2\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -43.6      50.8  17656206.0\n",
      "tmin         -47.5      30.3  17656206.0\n",
      "Maximum value summary (all scenarios) for model CCSM4\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -45.2      48.3  17656206.0\n",
      "tmin         -46.5      28.0  17656206.0\n",
      "Maximum value summary (all scenarios) for model CSIRO-Mk3-6-0\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -45.6      44.8  17656206.0\n",
      "tmin         -44.8      35.1  17656206.0\n",
      "Maximum value summary (all scenarios) for model GFDL-ESM2M\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -44.8      42.8  17656206.0\n",
      "tmin         -46.9      29.7  17656206.0\n",
      "Maximum value summary (all scenarios) for model HadGEM2-ES\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -42.8      47.8  17656206.0\n",
      "tmin         -43.7      37.1  17656206.0\n",
      "Maximum value summary (all scenarios) for model inmcm4\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -43.6      43.4  17656206.0\n",
      "tmin         -47.8      32.2  17656206.0\n",
      "Maximum value summary (all scenarios) for model MIROC5\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -43.4      44.0  17656206.0\n",
      "tmin         -46.1      30.7  17656206.0\n",
      "Maximum value summary (all scenarios) for model MPI-ESM-MR\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -42.8      44.9  17656206.0\n",
      "tmin         -47.5      29.9  17656206.0\n",
      "Maximum value summary (all scenarios) for model MRI-CGCM3\n",
      "          min_vals  max_vals   nan_count\n",
      "variable                                \n",
      "tmax         -45.0      44.3  17656206.0\n",
      "tmin         -49.4      28.8  17656206.0\n"
     ]
    }
   ],
   "source": [
    "for k in summary_stat_di.keys():\n",
    "    print(f\"Maximum value summary (all scenarios) for model {k}\")\n",
    "    mindf = summary_stat_di[k].drop([\"filename\"], axis=1).groupby(\"variable\").max().round(1)\n",
    "    print(mindf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add71ce-15d1-40d8-994e-226b33d8aebb",
   "metadata": {},
   "source": [
    "The minimums of the min-mean-max-value extractions look OK. Although we can see there are some very extreme temps here - the coldest tmax and tmin temps in this entire dataset are about -71°C, which is about -96°F! However, that could be realistic for January on top of Denali or something like that. It is unexpected that the coldest tmax is actually colder than the coldest tmin - but it is close enough that might just be a downscaling / bias correction artifact (it is a known possible issue that we've seen elsewhere). Good to see a stable minimum nan_count (other than HadGEM2-ES for which we already established is missing data) - put another way, the maximum data extent for each of these variables is very likely indentical.\n",
    "The hottest daily max temperatures in the entire dataset are definitely scary (~50°C or 122°F). Ulimately we'll be creating an average daily temperature dataset which will moderate these minima and maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf524be5-13b9-4794-a03f-b383faf3e519",
   "metadata": {},
   "source": [
    "## EDA Takeaways\n",
    "* data are generally homogenous\n",
    "* Discard HadGEM2-ES for now\n",
    "* the data variables `tmin` and `tmax` extreme values that are reasonable, but they are worth noting, though we'll squash this variability a bit when we create a `tavg` data variable\n",
    "* these data contain leap years\n",
    "* if you are plotting slices of the source data, beware that the auto-labeling will be incorrect for tmin/tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd665149-0b80-4bb3-99bd-30d403b1cf24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
